{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139cf9cb-1268-47e6-b495-489f99b2cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0f8a8-b95c-4335-a5e1-a2d6f4a31fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from app.vjepa_droid.transforms import make_transforms\n",
    "from utils.mpc_utils import (\n",
    "    compute_new_pose,\n",
    "    poses_to_diff\n",
    ")\n",
    "from transformers import UMT5EncoderModel, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from src.models.ac_predictor import vit_ac_predictor\n",
    "from src.models.vision_transformer import vit_giant_xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = UMT5EncoderModel.from_pretrained(\n",
    "    \"Wan-AI/Wan2.1-T2V-14B-Diffusers\", subfolder=\"text_encoder\", torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Wan-AI/Wan2.1-T2V-14B-Diffusers\", subfolder=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baab712-266c-4822-b229-673ef728abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VJEPA 2-AC model\n",
    "encoder = vit_giant_xformers(\n",
    "    img_size=256,\n",
    "    patch_size=16,\n",
    "    num_frames=512,\n",
    "    tubelet_size=2,\n",
    "    uniform_power=True,\n",
    "    use_sdpa=True,\n",
    "    use_silu=False,\n",
    "    wide_silu=True,\n",
    "    use_activation_checkpointing=False,\n",
    "    use_rope=True,\n",
    ").eval().to(\"cuda\")\n",
    "predictor = vit_ac_predictor(\n",
    "        img_size=256,\n",
    "        patch_size=16,\n",
    "        num_frames=512,\n",
    "        tubelet_size=2,\n",
    "        embed_dim=encoder.embed_dim,\n",
    "        predictor_embed_dim=1024,\n",
    "        depth=24,\n",
    "        is_frame_causal=True,\n",
    "        num_heads=16,\n",
    "        uniform_power=True,\n",
    "        use_rope=True,\n",
    "        use_sdpa=True,\n",
    "        use_silu=False,\n",
    "        wide_silu=True,\n",
    "        use_activation_checkpointing=False,\n",
    "    ).eval().to(\"cuda\")\n",
    "# This is the agibot checkpoint\n",
    "state_dict = torch.load(\"/mnt/weka/home/yi.gu/tokenizer/zh/vjepa2/results/bridge/4.8.vitg16-256px-8f_25_07_03_16_41_00/e275.pt\")\n",
    "# This is the language table dataset\n",
    "#state_dict = torch.load(\"/mnt/weka/home/yi.gu/tokenizer/zh/vjepa2/results/bridge/4.8.vitg16-256px-8f_25_07_03_16_40_36/e375.pt\")\n",
    "renamed_encoder_state_dict = {}\n",
    "for k, v in state_dict[\"encoder\"].items():\n",
    "    if k.startswith(\"module.\"):\n",
    "        renamed_encoder_state_dict[k[7:]] = v\n",
    "    else:\n",
    "        renamed_encoder_state_dict[k] = v\n",
    "\n",
    "renamed_predictor_state_dict = {}\n",
    "for k, v in state_dict[\"predictor\"].items():\n",
    "    if k.startswith(\"module.\"):\n",
    "        renamed_predictor_state_dict[k[7:]] = v\n",
    "    else:\n",
    "        renamed_predictor_state_dict[k] = v\n",
    "\n",
    "encoder.load_state_dict(renamed_encoder_state_dict)\n",
    "predictor.load_state_dict(renamed_predictor_state_dict)\n",
    "\n",
    "# Initialize transform\n",
    "crop_size = 256\n",
    "tokens_per_frame = int((crop_size // encoder.patch_size) ** 2)\n",
    "transform = make_transforms(\n",
    "    random_horizontal_flip=False,\n",
    "    random_resize_aspect_ratio=(1., 1.),\n",
    "    random_resize_scale=(1., 1.),\n",
    "    reprob=0.,\n",
    "    auto_augment=False,\n",
    "    motion_shift=False,\n",
    "    crop_size=crop_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01dc4f05-14b9-4e06-bdc0-cf61bd88e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_predictor(_z, _t):\n",
    "    _z = predictor(_z, _t)\n",
    "    _z = F.layer_norm(_z, (_z.size(-1),))\n",
    "    return _z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47dd6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from decord import VideoReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c78e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "#just using bridge data for a sample. You only need a first image and a text to do inference\n",
    "with open(\"/mnt/weka/home/yi.gu/world-model/evaluation/bridge/output_video0622/index.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "sample = data[0]\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66be50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_instruction = sample[\"instruction\"]\n",
    "text_instruction = tokenizer(text_instruction, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "text_input_ids, mask = text_instruction.input_ids[0], text_instruction.attention_mask[0]\n",
    "text_input_ids = text_input_ids.to(\"cuda\").unsqueeze(0)\n",
    "mask = mask.to(\"cuda\").unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    encoded_text = text_encoder(text_input_ids, attention_mask=mask).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8cc86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/mnt/weka/home/yi.gu/world-model/evaluation/bridge/output_video0622/\" + sample['video']\n",
    "loaded_video_clip = VideoReader(video_path)\n",
    "first_image = np.expand_dims(loaded_video_clip[0].asnumpy(), axis=0)\n",
    "first_image = transform(first_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "521714a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_target(c):\n",
    "    batch_size = c.shape[0]\n",
    "    with torch.no_grad():\n",
    "        c = c.permute(0, 2, 1, 3, 4).flatten(0, 1).unsqueeze(2).repeat(1, 1, 2, 1, 1)\n",
    "        h = encoder(c)\n",
    "        h = h.view(batch_size, 1, -1, h.size(-1)).flatten(1, 2)\n",
    "        h = F.layer_norm(h, (h.size(-1),))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8674d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image = forward_target(first_image.unsqueeze(0).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c449af",
   "metadata": {},
   "outputs": [],
   "source": [
    "_z = first_image\n",
    "encoded_text = encoded_text.to(torch.float32)\n",
    "for n in range(10):\n",
    "    _z_nxt = step_predictor(_z, encoded_text)[:, -256:]\n",
    "    _z = torch.cat([_z, _z_nxt], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9de6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 11\n",
    "n = 256\n",
    "frame_by_frame_representation = _z.reshape(1, t, n, -1)\n",
    "frame_by_frame_representation.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vjepa2-312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
